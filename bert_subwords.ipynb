{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc9e0a69",
   "metadata": {},
   "source": [
    "## About\n",
    "\n",
    "One of the things that makes BERT so flexible is its ability to handle out of vocabulary (OOV) words. When the model comes across a word that isn't in its vocabulary, it breaks that word into different \"subwords\" that _are_ in the vocabulary. These subwords become the tokenized representation of the word.\n",
    "\n",
    "But how many ways can a word be chopped up? What if we prevented BERT from ever using whole words? In what ways might a word's subwords differ or relate to one another in the embedding space? What, in short, would the embedding space of subwords look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6a845b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations, chain\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a9ee79",
   "metadata": {},
   "source": [
    "Load and intialize the tokenizer and model. Get all subwords from the tokenizer vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d294d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad6a8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_vocab = [token for token in tokenizer.get_vocab().keys() if \"#\" in token]\n",
    "hash_dict = hash_dict = {re.sub(r'#{1,3}', '', subword): subword for subword in sub_vocab}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f948015",
   "metadata": {},
   "source": [
    "Setup all the functions for finding and encoding every possible subword combination of a given word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d06946b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition(word):\n",
    "    n = len(word)\n",
    "    b, mid, e = [0], list(range(1, n)), [n]\n",
    "    splits = (split for i in range(n) for split in combinations(mid, i))\n",
    "    partitions = [[word[sl] for sl in map(slice, chain(b, split), chain(split, e))] for split in splits]\n",
    "    return partitions\n",
    "\n",
    "def get_subwords(word, hash_dict):\n",
    "    subwords = []\n",
    "    for subword in hash_dict.keys():\n",
    "        if subword in word:\n",
    "            hash_token = hash_dict[subword]\n",
    "            found = re.search('#{1,3}\\w+', hash_token)\n",
    "            if found is not None:\n",
    "                subwords.append(hash_token)\n",
    "    return subwords\n",
    "\n",
    "def check_partitions(partitions, subwords, hash_dict):\n",
    "    valid_tokens = []\n",
    "    for partition in partitions:\n",
    "        try:\n",
    "            valid_tokens.append(hash_dict[p] for p in partition)\n",
    "        except:\n",
    "            continue\n",
    "    return valid_tokens\n",
    "\n",
    "def pretty_print(valid_tokens):\n",
    "    return {'-'.join([re.sub('#', '', p) for p in partition]): partition for partition in valid_tokens}\n",
    "\n",
    "def append_special_tokens(valid_tokens):\n",
    "    return {subword: ['[CLS]'] + tokens + ['[SEP]'] for subword, tokens in valid_tokens.items()}\n",
    "\n",
    "def encode(tokenized, pad_len=15):\n",
    "    for subword, tokens in tokenized.items():\n",
    "        token_ids = [tokenizer.convert_tokens_to_ids(token) for token in tokens]\n",
    "        if len(tokens) < pad_len:\n",
    "            to_pad = [0] * (pad_len - len(tokens))\n",
    "            token_ids = token_ids + to_pad\n",
    "        token_type_ids = [0] * pad_len\n",
    "        attention_mask = [1] * len(tokens) + to_pad\n",
    "        tokenized[subword] = {\n",
    "            'tokens': tokens,\n",
    "            'input_ids': token_ids,\n",
    "            'token_type_ids': token_type_ids,\n",
    "            'attention_mask': attenton_mask\n",
    "        }\n",
    "    return tokenized\n",
    "\n",
    "def stack_tokenized(tokenized, convert_to_tensors=True):\n",
    "    stacked = {'input_ids': [], 'token_type_ids': [], 'attention_mask': []}\n",
    "    for subwords in tokenized.keys():\n",
    "        stacked['input_ids'].append(tokenized[subwords]['input_ids'])\n",
    "        stacked['token_type_ids'].append(tokenized[subwords]['token_type_ids'])\n",
    "        stacked['attention_mask'].append(tokenized[subwords]['attention_mask'])\n",
    "    \n",
    "    if convert_to_tensors == True:\n",
    "        stacked = {component: torch.tensor(stacked[component]) for component in stacked.keys()}\n",
    "    \n",
    "    return stacked, list(tokenized.keys())\n",
    "\n",
    "def prepare(word, pad_len=15):\n",
    "    partitions, subword = partition(word), get_subwords(word)\n",
    "    valid_tokens = check_partitions(partitions, subwords, hash_dict)\n",
    "    valid_tokens = pretty_print(valid_tokens)\n",
    "    tokenized = append_special_tokens(valid_tokens)\n",
    "    tokenized = encode(tokenized, pad_len=pad_len)\n",
    "    to_model, substrings = stack_tokenized(tokenized)\n",
    "    return to_model, substrings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e48300a",
   "metadata": {},
   "source": [
    "Transform the output of a model into graphable embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a9e826",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooled(output, to_model):\n",
    "    embeddings = output.last_hidden_state\n",
    "    mask = to_model['attention_mask'].unsqueeze(-1).expand(embeddings.size()).float()\n",
    "    masked_embeddings = embeddings * mask    \n",
    "    summed = torch.sum(masked_embeddings, 1)\n",
    "    summed_mask = torch.clamp(mask.sum(1), min=1e-9)\n",
    "    return summed / summed_masked\n",
    "\n",
    "def normalize(mean_pooled):\n",
    "    detached_tensors = mean_pooled.detach().cpu().numpy()\n",
    "    norm = np.linalg.norm(detached_tensors)\n",
    "    if norm == 0:\n",
    "        return detached_tensors\n",
    "    return detached_tensors / norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5fb735",
   "metadata": {},
   "source": [
    "Select a word and run all of the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d85f5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"arrogant\"\n",
    "to_model, substrings = prepare(word)\n",
    "\n",
    "print(len(substrings), \"permutations to model\")\n",
    "\n",
    "output = model(**to_model)\n",
    "mean_pooled = mean_pooled(output, to_model)\n",
    "normalized = normalize(mean_pooled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ac275f",
   "metadata": {},
   "source": [
    "Graph the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0b1db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_embeddings(mean_pooled, substrings):\n",
    "    tsne = TSNE(n_components=2, metric='cosine', init='pca', n_iter=1500)\n",
    "    bert_tsne = tsne.fit_transform(mean_pooled.detach().numpy())\n",
    "    \n",
    "    fig = plt.figure(figsize=(12,9))\n",
    "    points = []\n",
    "    for idx, substring in enumerate(substrings):\n",
    "        plt.scatter(bert_tsne[idx,0], bert_tsne[idx,1], s=0)\n",
    "        text = plt.text(bert_tsne[idx,0], bert_tsne[idx,1], substring, family='sans-serif')\n",
    "        points.append(text)\n",
    "        \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7e2696",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = graph_embeddings(mean_pooled, substrings)\n",
    "fig"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
