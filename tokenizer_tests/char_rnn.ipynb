{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23855f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import json\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af5c07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(object):   \n",
    "    \"\"\"This is a minimal character-level Vanilla RNN model, written by \n",
    "    Andrej Karpathy.\n",
    "    \n",
    "    Original: https://gist.github.com/karpathy/d4dee566867f8291f086\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, filepath, hidden_size, seq_length, learning_rate):\n",
    "        fin = open(filepath, 'r')\n",
    "        self.data = fin.read()\n",
    "        self.chars = list(set(self.data))\n",
    "        self.data_size = len(self.data)\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.char_to_idx = {ch: idx for idx, ch in enumerate(self.chars)}\n",
    "        self.idx_to_char = {idx: ch for idx, ch in enumerate(self.chars)}\n",
    "        fin.close()\n",
    "        \n",
    "        # size of the hidden layer\n",
    "        self.hidden_size = hidden_size\n",
    "        # number of steps to unroll the RNN for\n",
    "        self.seq_length = seq_length\n",
    "        self.learning_rate = learning_rate\n",
    "        self.training_info = {'iter': [], 'loss': [], 'perp': []}\n",
    "        \n",
    "        # input to hidden\n",
    "        self.Wxh = np.random.randn(self.hidden_size, self.vocab_size) * 0.01\n",
    "        # hidden to hidden\n",
    "        self.Whh = np.random.randn(self.hidden_size, self.hidden_size) * 0.01\n",
    "        # hidden to output\n",
    "        self.Why = np.random.randn(self.vocab_size, self.hidden_size) * 0.01\n",
    "        # hidden bias\n",
    "        self.bh = np.zeros((self.hidden_size, 1))\n",
    "        # output bias\n",
    "        self.by = np.zeros((self.vocab_size, 1))\n",
    "\n",
    "    def loss(self, inputs, targets, hprev):\n",
    "        \"\"\"Calculate the loss for a pass.\n",
    "        \n",
    "        targets and inputs are lists of integers (chars from the training data)\n",
    "        hprev is Hx1 array of initial hidden state\n",
    "        returns the loss, gradients of the model parameters, and the last hidden state\n",
    "        \"\"\"\n",
    "        xs, hs, ys, ps = {}, {}, {}, {}\n",
    "        hs[-1] = np.copy(hprev)\n",
    "        loss = 0\n",
    "        \n",
    "        # forward pass\n",
    "        for t in range(len(inputs)):\n",
    "            # encode in 1-of-k representation\n",
    "            xs[t] = np.zeros((self.vocab_size, 1))\n",
    "            xs[t][inputs[t]] = 1\n",
    "            # hidden state\n",
    "            hs[t] = np.tanh(\n",
    "                np.dot(self.Wxh, xs[t]) + \n",
    "                np.dot(self.Whh, hs[t - 1]) + \n",
    "                self.bh\n",
    "            )\n",
    "            # unnormalized log probabilities for next chars\n",
    "            ys[t] = np.dot(self.Why, hs[t]) + self.by\n",
    "            # probabilities for next chars\n",
    "            ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t]))\n",
    "            # softmax (cross-entropy loss)\n",
    "            loss += -np.log(ps[t][targets[t], 0])\n",
    "        \n",
    "        # backwards pass\n",
    "        dWxh = np.zeros_like(self.Wxh)\n",
    "        dWhh = np.zeros_like(self.Whh)\n",
    "        dWhy = np.zeros_like(self.Why)\n",
    "        dbh = np.zeros_like(self.bh)\n",
    "        dby = np.zeros_like(self.by)\n",
    "        dhnext = np.zeros_like(hs[0])\n",
    "        \n",
    "        for t in reversed(range(len(inputs))):\n",
    "            dy = np.copy(ps[t])\n",
    "            # backprop into y\n",
    "            dy[targets[t]] -= 1\n",
    "            dWhy += np.dot(dy, hs[t].T)\n",
    "            dby += dy\n",
    "            # backprop into h\n",
    "            dh = np.dot(self.Why.T, dy) + dhnext\n",
    "            # backprop through tanh nonlinearity\n",
    "            dhraw = (1 - hs[t] * hs[t]) * dh\n",
    "            dbh += dhraw\n",
    "            dWxh += np.dot(dhraw, xs[t].T)\n",
    "            dWhh += np.dot(dhraw, hs[t - 1].T)\n",
    "            dhnext = np.dot(self.Whh.T, dhraw)\n",
    "        \n",
    "        # clip to mitigate exploding gradients\n",
    "        for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "            np.clip(dparam, -5, 5, out=dparam)\n",
    "            \n",
    "        return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs) - 1]\n",
    "    \n",
    "    def sample(self, h, seed_idx, n_char):\n",
    "        \"\"\"Sample a sequence of integers from the model.\n",
    "        \n",
    "        h is memory state, seed_idx is the seed letter for the first time step\n",
    "        n_char is the number of characters to sample\n",
    "        \"\"\"\n",
    "        x = np.zeros((self.vocab_size, 1))\n",
    "        x[seed_idx] = 1\n",
    "        idxes = []\n",
    "        for t in range(n_char):\n",
    "            h = np.tanh(np.dot(self.Wxh, x) + np.dot(self.Whh, h) + self.bh)\n",
    "            y = np.dot(self.Why, h) + self.by\n",
    "            p = np.exp(y) / np.sum(np.exp(y))\n",
    "            idx = np.random.choice(range(self.vocab_size), p=p.ravel())\n",
    "            x = np.zeros((self.vocab_size, 1))\n",
    "            x[idx] = 1\n",
    "            idxes.append(idx)\n",
    "            \n",
    "        return idxes\n",
    "    \n",
    "    def train(self, sample_rate=100, sample_size=200):\n",
    "        \"\"\"Train the model.\"\"\"\n",
    "        n, p = 0, 0\n",
    "        mWxh = np.zeros_like(self.Wxh)\n",
    "        mWhh = np.zeros_like(self.Whh)\n",
    "        mWhy = np.zeros_like(self.Why)\n",
    "        # memory variables for Adagrad\n",
    "        mbh = np.zeros_like(self.bh)\n",
    "        mby = np.zeros_like(self.by)\n",
    "        # loss at iteration 0\n",
    "        smooth_loss = -np.log(1.0 / self.vocab_size) * self.seq_length\n",
    "        \n",
    "        while True:\n",
    "            # prepare inputs (sweeps from left->right in steps seq_length long)\n",
    "            if p + self.seq_length + 1 >= self.data_size or n == 0:\n",
    "                # reset RNN memory\n",
    "                hprev = np.zeros((self.hidden_size, 1))\n",
    "                # go to start of the data\n",
    "                p = 0\n",
    "                \n",
    "            inputs = [self.char_to_idx[ch] for ch in self.data[p:p+self.seq_length]]\n",
    "            targets = [self.char_to_idx[ch] for ch in self.data[p+1:p+self.seq_length+1]]\n",
    "            \n",
    "            # sample for logging purposes\n",
    "            if n % sample_rate == 0:\n",
    "                sample_idx = self.sample(hprev, inputs[0], sample_size)\n",
    "                txt = ''.join(self.idx_to_char[idx] for idx in sample_idx)\n",
    "                print(f\"=======\\n{txt}\\n=======\\n\")\n",
    "            \n",
    "            # forward seq_length characters through the net and fetch gradient\n",
    "            loss, dWxh, dWhh, dWhy, dbh, dby, hprev = self.loss(inputs, targets, hprev)\n",
    "            smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "            # print progress and\n",
    "            if n % sample_rate == 0:\n",
    "                # calculate perplexity, which can apparently be done from the cross-entropy, as per\n",
    "                # https://stackoverflow.com/questions/61988776/how-to-calculate-perplexity-for-a-language-model-using-pytorch\n",
    "                perplexity = np.exp(smooth_loss)\n",
    "                self.training_info['iter'].append(n)\n",
    "                self.training_info['loss'].append(smooth_loss)\n",
    "                self.training_info['perp'].append(perplexity)\n",
    "                print(f\"+ iter {n}\\n+ loss: {smooth_loss:0.6f}\\n+ perplexity: {perplexity:0.6f}\\n\")\n",
    "            \n",
    "            # perform parameter update with Adagrad\n",
    "            for param, dparam, mem in zip(\n",
    "                [self.Wxh, self.Whh, self.Why, self.bh, self.by],\n",
    "                [dWxh, dWhh, dWhy, dbh, dby],\n",
    "                [mWxh, mWhh, mWhy, mbh, mby]\n",
    "            ):\n",
    "                mem += dparam * dparam\n",
    "                # adagrad update\n",
    "                param += -self.learning_rate * dparam / np.sqrt(mem + 1e-8)\n",
    "            \n",
    "            # move data pointer\n",
    "            p += self.seq_length\n",
    "            # iteration counter\n",
    "            n += 1\n",
    "            \n",
    "    def plot_performance(self, metric=None, size=10):\n",
    "        chunks = zip(\n",
    "            np.array_split(self.training_info['iter'], size),\n",
    "            np.array_split(self.training_info[metric], size)\n",
    "        )\n",
    "        graph_data = [(a[0], b[0]) for a, b in chunks]\n",
    "        x = [i[0] for i in graph_data]\n",
    "        y = [i[1] for i in graph_data]\n",
    "        fig = plt.figure(figsize=(15,9))\n",
    "        plt.plot(x, y)\n",
    "        plt.ticklabel_format(style='plain');\n",
    "        plt.savefig(metric + '.png', format='png')\n",
    "        \n",
    "    def save_training_info(self, filepath):\n",
    "        with open(filepath, 'w') as j:\n",
    "            json.dump(self.training_info, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d35fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN('sherlock.txt', 100, 25, 1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16fb218",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rnn.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8e6aed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
