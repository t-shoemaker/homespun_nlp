{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23855f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import json\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4af5c07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(object):   \n",
    "    \"\"\"This is a minimal character-level Vanilla RNN model, written by Andrej Karpathy.\n",
    "    \n",
    "    Original: https://gist.github.com/karpathy/d4dee566867f8291f086\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, filepath, hidden_size, seq_length, learning_rate):\n",
    "        fin = open(filepath, 'r')\n",
    "        self.data = fin.read()\n",
    "        self.chars = list(set(self.data))\n",
    "        self.data_size = len(self.data)\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.char_to_idx = {ch: idx for idx, ch in enumerate(self.chars)}\n",
    "        self.idx_to_char = {idx: ch for idx, ch in enumerate(self.chars)}\n",
    "        fin.close()\n",
    "        \n",
    "        # size of the hidden layer\n",
    "        self.hidden_size = hidden_size\n",
    "        # number of steps to unroll the RNN for\n",
    "        self.seq_length = seq_length\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # input to hidden\n",
    "        self.Wxh = np.random.randn(self.hidden_size, self.vocab_size) * 0.01\n",
    "        # hidden to hidden\n",
    "        self.Whh = np.random.randn(self.hidden_size, self.hidden_size) * 0.01\n",
    "        # hidden to output\n",
    "        self.Why = np.random.randn(self.vocab_size, self.hidden_size) * 0.01\n",
    "        # hidden bias\n",
    "        self.bh = np.zeros((self.hidden_size, 1))\n",
    "        # output bias\n",
    "        self.by = np.zeros((self.vocab_size, 1))\n",
    "        \n",
    "        # static loss score\n",
    "        self.loss = 0\n",
    "        # static perplexity score\n",
    "        self.perplexity = 0\n",
    "        # tracking all the metrics over the course of training\n",
    "        self.training_info = {'iter': [], 'loss': [], 'perp': []}\n",
    "        # a static hidden state for character generation\n",
    "        self.hprev = np.zeros((self.hidden_size, 1))\n",
    "\n",
    "    def calculate_loss(self, inputs, targets, hprev):\n",
    "        \"\"\"Calculate the loss for a pass.\n",
    "        \n",
    "        targets and inputs are lists of integers (chars from the training data)\n",
    "        hprev is Hx1 array of initial hidden state\n",
    "        returns the loss, gradients of the model parameters, and the last hidden state\n",
    "        \"\"\"\n",
    "        xs, hs, ys, ps = {}, {}, {}, {}\n",
    "        hs[-1] = np.copy(hprev)\n",
    "        loss = 0\n",
    "        \n",
    "        # forward pass\n",
    "        for t in range(len(inputs)):\n",
    "            # encode in 1-of-k representation\n",
    "            xs[t] = np.zeros((self.vocab_size, 1))\n",
    "            xs[t][inputs[t]] = 1\n",
    "            # hidden state\n",
    "            hs[t] = np.tanh(\n",
    "                np.dot(self.Wxh, xs[t]) + \n",
    "                np.dot(self.Whh, hs[t - 1]) + \n",
    "                self.bh\n",
    "            )\n",
    "            # unnormalized log probabilities for next chars\n",
    "            ys[t] = np.dot(self.Why, hs[t]) + self.by\n",
    "            # probabilities for next chars\n",
    "            ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t]))\n",
    "            # softmax (cross-entropy loss)\n",
    "            loss += -np.log(ps[t][targets[t], 0])\n",
    "        \n",
    "        # backwards pass\n",
    "        dWxh = np.zeros_like(self.Wxh)\n",
    "        dWhh = np.zeros_like(self.Whh)\n",
    "        dWhy = np.zeros_like(self.Why)\n",
    "        dbh = np.zeros_like(self.bh)\n",
    "        dby = np.zeros_like(self.by)\n",
    "        dhnext = np.zeros_like(hs[0])\n",
    "        \n",
    "        for t in reversed(range(len(inputs))):\n",
    "            dy = np.copy(ps[t])\n",
    "            # backprop into y\n",
    "            dy[targets[t]] -= 1\n",
    "            dWhy += np.dot(dy, hs[t].T)\n",
    "            dby += dy\n",
    "            # backprop into h\n",
    "            dh = np.dot(self.Why.T, dy) + dhnext\n",
    "            # backprop through tanh nonlinearity\n",
    "            dhraw = (1 - hs[t] * hs[t]) * dh\n",
    "            dbh += dhraw\n",
    "            dWxh += np.dot(dhraw, xs[t].T)\n",
    "            dWhh += np.dot(dhraw, hs[t - 1].T)\n",
    "            dhnext = np.dot(self.Whh.T, dhraw)\n",
    "        \n",
    "        # clip to mitigate exploding gradients\n",
    "        for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "            np.clip(dparam, -5, 5, out=dparam)\n",
    "            \n",
    "        return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs) - 1]\n",
    "    \n",
    "    def sample(self, h, seed_idx, n_char):\n",
    "        \"\"\"Sample a sequence of integers from the model.\n",
    "        \n",
    "        h is memory state, seed_idx is the seed letter for the first time step\n",
    "        n_char is the number of characters to sample\n",
    "        \"\"\"\n",
    "        x = np.zeros((self.vocab_size, 1))\n",
    "        x[seed_idx] = 1\n",
    "        idxes = []\n",
    "        for t in range(n_char):\n",
    "            h = np.tanh(np.dot(self.Wxh, x) + np.dot(self.Whh, h) + self.bh)\n",
    "            y = np.dot(self.Why, h) + self.by\n",
    "            p = np.exp(y) / np.sum(np.exp(y))\n",
    "            idx = np.random.choice(range(self.vocab_size), p=p.ravel())\n",
    "            x = np.zeros((self.vocab_size, 1))\n",
    "            x[idx] = 1\n",
    "            idxes.append(idx)\n",
    "            \n",
    "        return idxes\n",
    "    \n",
    "    def train(self, sample_rate=100, sample_size=200):\n",
    "        \"\"\"Train the model.\"\"\"\n",
    "        n, p = 0, 0\n",
    "        mWxh = np.zeros_like(self.Wxh)\n",
    "        mWhh = np.zeros_like(self.Whh)\n",
    "        mWhy = np.zeros_like(self.Why)\n",
    "        # memory variables for Adagrad\n",
    "        mbh = np.zeros_like(self.bh)\n",
    "        mby = np.zeros_like(self.by)\n",
    "        # loss at iteration 0\n",
    "        smooth_loss = -np.log(1.0 / self.vocab_size) * self.seq_length\n",
    "        \n",
    "        while True:\n",
    "            # prepare inputs (sweeps from left->right in steps seq_length long)\n",
    "            if p + self.seq_length + 1 >= self.data_size or n == 0:\n",
    "                # reset RNN memory\n",
    "                hprev = np.zeros((self.hidden_size, 1))\n",
    "                # go to start of the data\n",
    "                p = 0\n",
    "                \n",
    "            inputs = [self.char_to_idx[ch] for ch in self.data[p:p+self.seq_length]]\n",
    "            targets = [self.char_to_idx[ch] for ch in self.data[p+1:p+self.seq_length+1]]\n",
    "            \n",
    "            # sample for logging purposes\n",
    "            if n % sample_rate == 0:\n",
    "                sample_idx = self.sample(hprev, inputs[0], sample_size)\n",
    "                txt = ''.join(self.idx_to_char[idx] for idx in sample_idx)\n",
    "                print(f\"=======\\n{txt}\\n=======\\n\")\n",
    "            \n",
    "            # forward seq_length characters through the net and fetch gradient\n",
    "            loss, dWxh, dWhh, dWhy, dbh, dby, hprev = self.calculate_loss(inputs, targets, hprev)\n",
    "            smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "            # print progress and\n",
    "            if n % sample_rate == 0:\n",
    "                # update the static hidden state\n",
    "                self.hprev = np.copy(hprev)\n",
    "                # calculate perplexity, which can apparently be done from the cross-entropy, as per\n",
    "                # https://stackoverflow.com/questions/61988776/how-to-calculate-perplexity-for-a-language-model-using-pytorch\n",
    "                self.perplexity = np.exp(smooth_loss)\n",
    "                self.loss = smooth_loss\n",
    "                \n",
    "                self.training_info['iter'].append(n)\n",
    "                self.training_info['loss'].append(self.loss)\n",
    "                self.training_info['perp'].append(self.perplexity)\n",
    "                print(f\"+ iter {n}\\n+ loss: {self.loss:0.6f}\\n+ perplexity: {self.perplexity:0.6f}\\n\")\n",
    "            \n",
    "            # perform parameter update with Adagrad\n",
    "            for param, dparam, mem in zip(\n",
    "                [self.Wxh, self.Whh, self.Why, self.bh, self.by],\n",
    "                [dWxh, dWhh, dWhy, dbh, dby],\n",
    "                [mWxh, mWhh, mWhy, mbh, mby]\n",
    "            ):\n",
    "                mem += dparam * dparam\n",
    "                # adagrad update\n",
    "                param += -self.learning_rate * dparam / np.sqrt(mem + 1e-8)\n",
    "            \n",
    "            # move data pointer\n",
    "            p += self.seq_length\n",
    "            # iteration counter\n",
    "            n += 1\n",
    "            \n",
    "    def generate(self, char, n_chars=200):\n",
    "        char_idx = self.char_to_idx[char]\n",
    "        sample_idx = self.sample(self.hprev, char_idx, n_chars)\n",
    "        txt = char + ''.join(self.idx_to_char[idx] for idx in sample_idx)\n",
    "        return txt\n",
    "            \n",
    "    def plot_performance(self, metric=None, size=10):\n",
    "        chunks = zip(\n",
    "            np.array_split(self.training_info['iter'], size),\n",
    "            np.array_split(self.training_info[metric], size)\n",
    "        )\n",
    "        graph_data = [(a[0], b[0]) for a, b in chunks]\n",
    "        x = [i[0] for i in graph_data]\n",
    "        y = [i[1] for i in graph_data]\n",
    "        fig = plt.figure(figsize=(15,9))\n",
    "        plt.plot(x, y)\n",
    "        plt.ticklabel_format(style='plain');\n",
    "        plt.savefig(metric + '.png', format='png')\n",
    "        \n",
    "    def save_training_info(self, filepath):\n",
    "        with open(filepath, 'w') as j:\n",
    "            json.dump(self.training_info, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91d35fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = CharRNN('sherlock.txt', 100, 25, 1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a16fb218",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======\n",
      "¨yXs[R.:wka.`,xXCbÂn»g©ª.Ie3DrC™7EC4RŸxyŸr'½ŸyC[o'±7DNGâbÃ`u3AGBsi36opI 6®S;?QDdn&lM2M\"?F FÂ\"R!ynaR¢)(s\"t€;W.t015,rz K½'Ÿffob'¨HpC´1Tw J\"T»5Kd±ªw\n",
      "H?Ã;â*IB.hªaªtÃTaâI(8Dl€pCr[T`€yâ78U*p,?t4´£ICL»ªAfg(5\n",
      "=======\n",
      "\n",
      "+ iter 0\n",
      "+ loss: 114.877996\n",
      "+ perplexity: 77782117528988424807502672165404869547662877130752.000000\n",
      "\n",
      "=======\n",
      "ifcyturoEAm¶  teSg!fm  rrZ.mvb ue tr´Q2  urS\n",
      "lkg[YOug5rÂEtaeTS™gad TjdC€SG  ocQiiP'A  uecoRFA T eflyHXT ueEd\n",
      "HW t oTh  GJu¨lH°°)k0rSPTo&cHna&d'::vavaUtV[9:[   rl5-½EYhToi'VEoaTLTCp     eSRK?8hthDglw'e\n",
      "=======\n",
      "\n",
      "+ iter 100\n",
      "+ loss: 114.513897\n",
      "+ perplexity: 54044769966087274978766360554065207296122507231232.000000\n",
      "\n",
      "=======\n",
      "ereoSioimi x nn ale a  dd  ee ¼ rAfon  hIK.haawowwueealamdaokhtnhtB ,\n",
      " samao awEa kÂ  etneet  si ieve raNadav  aE esyi ar e\n",
      "o naadIHw aata tikdflte  abwulfnwaMaai,fe nca uEvne\n",
      " uzO  baIdee   o rgt wd\n",
      "\n",
      "=======\n",
      "\n",
      "+ iter 200\n",
      "+ loss: 111.965994\n",
      "+ perplexity: 4228762385449490726802591450840964056495733866496.000000\n",
      "\n",
      "=======\n",
      "nt .  \n",
      "h t \"  n oti dsuoampoa  faate\n",
      " ,\n",
      "d de\n",
      "Trietao  \n",
      " riff\"os, ar  auoi  ,amnt s o\"eei ldtpauenlH d ataereoiua utnm  lutoyde d e  g  uraa   yoa Iw\n",
      "eo sItmt,hu htyfo khatt\"g t ttgdotthp.bn hr  onsalc\n",
      "=======\n",
      "\n",
      "+ iter 300\n",
      "+ loss: 108.904148\n",
      "+ perplexity: 197911175953653585724754277365821508624802906112.000000\n",
      "\n",
      "=======\n",
      "n\"dDt'd adil\"Yl t it\n",
      "edlcbi httreIat9tlyc\n",
      " a us\"  unlveroai slaI rehl\"mdBe fncttm aof\"pM \n",
      "eml t gief \n",
      "tYIru-nutrco ent  mHbllePoIosrlm\"    tntocfCy'\n",
      "teed idye  yb nenoinoeyGBtt kledmt or roi kRd  vorp\n",
      "=======\n",
      "\n",
      "+ iter 400\n",
      "+ loss: 106.198636\n",
      "+ perplexity: 13227614904914321934326046278120394987835228160.000000\n",
      "\n",
      "=======\n",
      " FHweodan n ls hewba is sw\n",
      " onr.n\" ieram.ern d oeaenanieibs hano wh yt samydftjeqwlielsehswh\n",
      "\" pnsnwheo.l\n",
      "  w,l\n",
      "lpapie ano  yhenowdo  htl\"nh of usn ct,eue\"davs\n",
      " is df\n",
      "  wnenoue-lyiee   dcsoIht. naeg\n",
      " \n",
      "=======\n",
      "\n",
      "+ iter 500\n",
      "+ loss: 103.526225\n",
      "+ perplexity: 913836038411512099589179130497763427399761920.000000\n",
      "\n",
      "=======\n",
      "e\n",
      "orcsn S\n",
      "aldethlmv int\n",
      "  ruis, \"ceiusnteri  anp t i . ame ias ecare sao\n",
      " ure  ourrinwpsear n ay\n",
      " ennith scr , olorroeetoc rid  ercae ddtee  -' utrt undaTctuag )\n",
      "hs ine , efea.ratei u h tfnehsIodw le \n",
      "=======\n",
      "\n",
      "+ iter 600\n",
      "+ loss: 101.006400\n",
      "+ perplexity: 73539780775842963682739757232461395041189888.000000\n",
      "\n",
      "=======\n",
      "suu. . aa\n",
      "P ndar ts±h\n",
      " aaahbabe rolho onjnte ofns ia f are e lamdesswlauehhky tohealxouh mi\n",
      "s bio\n",
      " ad is mia so»\n",
      " ygtvt eat I  wsee cl,mot s eut e, ty ond\n",
      "o wev\"venen?utuwth . s ntrhewe Y r'o's seew, \n",
      "=======\n",
      "\n",
      "+ iter 700\n",
      "+ loss: 98.495287\n",
      "+ perplexity: 5969800867597267285499874528481630188732416.000000\n",
      "\n",
      "=======\n",
      "cile osId se mg eenadebtnciuron an inibrpe\n",
      " ed fal pm p\n",
      "r iu t\n",
      " zf hamette eranf mouecondi nL iopes li oin aowant Ihenw\"gboqtinko,aoye wdut ortnd ce s.inkpilso¶d g\n",
      "amg nnferothimouot\n",
      " Iine s p\n",
      "dread i\n",
      "=======\n",
      "\n",
      "+ iter 800\n",
      "+ loss: 96.150816\n",
      "+ perplexity: 572491438632840266339671645655466785112064.000000\n",
      "\n",
      "=======\n",
      "Shladih s. apifshcledate atepeot rt hived\n",
      " hHe t sg pyitrl kedimem, tuenunre\"ting goseovad h m'og se\n",
      " osd, ipa ior perubret conile\n",
      "e y ints oTy.  iv icote-a d nd,at matn yioath amd, is t.\n",
      "d ihhe uheri\n",
      "=======\n",
      "\n",
      "+ iter 900\n",
      "+ loss: 93.519226\n",
      "+ perplexity: 41198718295505108639266129087252339359744.000000\n",
      "\n",
      "=======\n",
      "tev\n",
      "gors We heri tadTd ajt an te  eomlt\"g te uls\n",
      " ns er \"erd hartetirs fhte aot esyatl Ieral streiore !lwe\n",
      "wapC hend cepsunftr tiwert ww datk \n",
      "yotsyodesIy. dt meosnhol\n",
      "evotit tov th is cfothofafts ato\n",
      "=======\n",
      "\n",
      "+ iter 1000\n",
      "+ loss: 91.175382\n",
      "+ perplexity: 3953348416670156332402455348176970842112.000000\n",
      "\n",
      "=======\n",
      "eteud scecthes. cegibd on g ol\n",
      ".\n",
      " .. tldesgy \n",
      "gsauimeisotpr d nerVa aLgrcty t ze\n",
      " ad, aneminbesiceeciiv far h Auntare sen ysd hyannis lsag tr ee be cad lt sos be eilseyactes annanld  onaon \"vennanay p\n",
      "=======\n",
      "\n",
      "+ iter 1100\n",
      "+ loss: 89.267524\n",
      "+ perplexity: 586668813046800289247058512826727399424.000000\n",
      "\n",
      "=======\n",
      "d, at de Sirm hd tem n iy at ioudon, darul tpon\n",
      " un\n",
      " tchas m yetre ut I aantet\n",
      " piltlu g une t kilicatangaW t tWot Sy wa ucthesdinotar.aranlf tels ar atlhatid plee cyaphe rtI taW oag me un he gap t ar\n",
      "=======\n",
      "\n",
      "+ iter 1200\n",
      "+ loss: 87.095733\n",
      "+ perplexity: 66864602811246090564918024712799387648.000000\n",
      "\n",
      "=======\n",
      "beanyhhamf hiosGI yiW ut7terlark tolhyud oshe hhonk5 thhart karHad ey ntIley pin iwy hoachly ciader, wolarurig arttWs ar he big ertrbinl an c tb\n",
      " hurf, fhounvicanti sisYans an lolint\n",
      " yh, lh aptyimbhh\n",
      "=======\n",
      "\n",
      "+ iter 1300\n",
      "+ loss: 85.156712\n",
      "+ perplexity: 9618122328570899415845056524920553472.000000\n",
      "\n",
      "=======\n",
      "cufy to laanmalde. wyovare thevos arkics\n",
      " asAdametGalplosan, \"n\n",
      "e mivhoo We YpiNse  anl au\n",
      "xing om st hopfinki\"oot ond he haiYgwall Ieos\n",
      " awcuas w in an ten tpto lesiy tise !s curons,de lbhadt e, brh \n",
      "=======\n",
      "\n",
      "+ iter 1400\n",
      "+ loss: 83.400517\n",
      "+ perplexity: 1661056140976779041557265173633302528.000000\n",
      "\n",
      "=======\n",
      "nasdicdow the ond hic onta\"sessamaape wan ase ag Pont nenmes\" b er chad omirc.lot .es rag win el af ie hat\n",
      " an sate\n",
      "e Putes.seine gam\n",
      " farecgoon iige brity cisrnes ores binol-pifacoid sun el ad hy toi\n",
      "=======\n",
      "\n",
      "+ iter 1500\n",
      "+ loss: 81.948800\n",
      "+ perplexity: 388965814463151557310124725106638848.000000\n",
      "\n",
      "=======\n",
      "tecasronr.Te Ma!.t sitar, h.\n",
      " rond tavhets ur !arewhanrePfib hhsiwkete-utelherd\"mamhe ter.edan ad\"saaln thend kowe s\" shed\n",
      "lanlhounkingininher ty\n",
      "ederinigese Ie wan te\n",
      "r vet plfen dltyons ls,doecoulri\n",
      "=======\n",
      "\n",
      "+ iter 1600\n",
      "+ loss: 80.525150\n",
      "+ perplexity: 93676029415608284133293177402556416.000000\n",
      "\n",
      "=======\n",
      "e inos h ersmen thhhtiard r iye bnl d arafaumliontare tevpele ofobde ondrtile I ed wo se\n",
      " siHe do v c zan!lhaau. yanmi-lu te  sand\n",
      " ovondu se Jle a phermtedesergenOerh.y r te, to\n",
      " irgese taund i\n",
      " an e\n",
      "=======\n",
      "\n",
      "+ iter 1700\n",
      "+ loss: 79.174454\n",
      "+ perplexity: 24267700504559742573427850892804096.000000\n",
      "\n",
      "=======\n",
      ", woed\n",
      " To et walcune toI buwkloutloHl to.ut io s abheraBp, thevomseldG.\n",
      " tene ossar The siboty sy hho\" fove \" Rkomimil7 me\". ufs ott douvann, I. dat hr, wtyt- in wy\n",
      " acge ou aud Aum\n",
      " me ig oF\n",
      " ort ry\n",
      "=======\n",
      "\n",
      "+ iter 1800\n",
      "+ loss: 77.932620\n",
      "+ perplexity: 7009818814209627993728424867790848.000000\n",
      "\n",
      "=======\n",
      "ne ribch, Held rat kint bas\n",
      " atiafd is, ho t\n",
      " wod Toty\" ic¨imothe tarbot the Bfhagloesed tm luhe s ene\n",
      "; winke\n",
      " ane\"thohaNs arhouxsti(cy wirhakf anaiclf  ofsoacxrifialin itateng cont fone ak fu oft an\n",
      "=======\n",
      "\n",
      "+ iter 1900\n",
      "+ loss: 76.658222\n",
      "+ perplexity: 1959939752705622401828161792770048.000000\n",
      "\n",
      "=======\n",
      "hired are ilud afsdocor it bhrhow ooces rwe sipSwe yesvhh hipsend hece , wipe dopHe bElho atet,\n",
      " o lhen aad ganm tif ad\" Lhe\n",
      " an ede Ter ohe ent or selaofd targand bt\n",
      " aft cf bas an, alg decharcoidin \n",
      "=======\n",
      "\n",
      "+ iter 2000\n",
      "+ loss: 75.534888\n",
      "+ perplexity: 637360222942145296404746378149888.000000\n",
      "\n",
      "=======\n",
      " on mos te. dan an Nir¨e cas ouurle ar set pchud\n",
      " en \"anY tert \"aykantige-n or ods oot shtrat anreede thane se. llen n m, whu\n",
      "en te rhe. endy me on tn iwteu. Thhes\n",
      ". es he ae -ant if mo nak\".\n",
      "d an, rh\n",
      "=======\n",
      "\n",
      "+ iter 2100\n",
      "+ loss: 74.398995\n",
      "+ perplexity: 204678948037247097253034746445824.000000\n",
      "\n",
      "=======\n",
      "s mhe bive nU, fran. ansank an bt bs aribolre tioqdolgte l, boathhirfar\n",
      " thhalanksirr oan. Apice-\"\n",
      "\n",
      " d ind bl are hos, und thoun I\n",
      ", hiacir tled sad ortea. bhovone\n",
      " anvy\n",
      " stroon p \n",
      "  ee  hyo.e c\" J?as\n",
      "=======\n",
      "\n",
      "+ iter 2200\n",
      "+ loss: 73.610507\n",
      "+ perplexity: 93032962971613363625673072050176.000000\n",
      "\n",
      "=======\n",
      "y en axtopavo ind\"-endter ond L thare betaev .apbul. \n",
      " dhind aks.seClt\n",
      " inn of dlirc thhe tis wan hon,\n",
      " us bcg eaid eA\n",
      " whe th ged bos pipr ro pife ar Aapat ne hooJ\n",
      "\n",
      " mof,as onke weas. an a rr ans fei\n",
      "=======\n",
      "\n",
      "+ iter 2300\n",
      "+ loss: 72.597871\n",
      "+ perplexity: 33795163970905238773808314187776.000000\n",
      "\n",
      "=======\n",
      "medert ovameit \"hand ilamoas,t. sha gherberot to Ie\n",
      " lz Ytoh is?tel\n",
      "  lat.ceaxgasy anLesoors kesseran\n",
      "\n",
      " Aiae hofmos alo kaaun\n",
      " oucle ayo sonlte aas\n",
      " dict \"Aged or ilyesials  sreaTherlime ¶cte bopexlid\n",
      "=======\n",
      "\n",
      "+ iter 2400\n",
      "+ loss: 71.765759\n",
      "+ perplexity: 14705279164479891520940064899072.000000\n",
      "\n",
      "=======\n",
      "iget chadeceatitf ha-eseref act ry tt by bhiong cony tiun.d ndhocf bk e w\" seater whocthalM ife; is.ece\n",
      "toude seur.\"elo bs acheh tw ang, ow''as thibat bo chas\n",
      "t rarge He sith and oasttine m th pale, t\n",
      "=======\n",
      "\n",
      "+ iter 2500\n",
      "+ loss: 70.918718\n",
      "+ perplexity: 6303876754300070223570517622784.000000\n",
      "\n",
      "=======\n",
      "acoubtigh met. wes Whecead uy rba Gs hicorres thos oly lanban ounc, haviul. inl- te yem wanpe \"Ranf fce thatpins -ott\" Ikemrcenteree yd orabn, whed. m, talt seeala sAn tor  He pum ane \"-int ortramy Ch\n",
      "=======\n",
      "\n",
      "+ iter 2600\n",
      "+ loss: 70.103466\n",
      "+ perplexity: 2789640893903641472078597586944.000000\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======\n",
      "ntur avgoun foadu thede\n",
      " tu Au mag ae mand\n",
      "\n",
      ",\n",
      "\n",
      "\n",
      " ad'smiboond\n",
      " wuns an rhe naod iechinherisnef urg ave gRet' ad thet bd tancard wlen iverd the\n",
      " are aad wonan. surey an tree surloco sus tarjannethiopad \n",
      "=======\n",
      "\n",
      "+ iter 2700\n",
      "+ loss: 69.404842\n",
      "+ perplexity: 1387203150807902113347066134528.000000\n",
      "\n",
      "=======\n",
      "towbof dam od desd Th,y ¢het ha1dov hed the sml an, an\n",
      " ece saf in ou fros worre tott iunt thychyen oheclo oWhenmithecy tho-e pites hhalk. hou whe the Towe\n",
      " sox?uumoumingero loum te feat wire ante onc\n",
      "=======\n",
      "\n",
      "+ iter 2800\n",
      "+ loss: 68.768433\n",
      "+ perplexity: 734093134967811994677913059328.000000\n",
      "\n",
      "=======\n",
      "et ser\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\" brat qnd as tfis, alin.n-\"G\n",
      " 'rde\n",
      " \"I che hei'nger I ,ad.\n",
      " cnHnoe womtippeiy fane te\"tepirinko\n",
      ". \"\n",
      "\n",
      " dwe thloouthet\n",
      " s icenindes hep ten feep. thum gu th soc. to this ,e \"me lat\n",
      " \"dsed ob\n",
      "=======\n",
      "\n",
      "+ iter 2900\n",
      "+ loss: 68.096780\n",
      "+ perplexity: 375021339202716371966532517888.000000\n",
      "\n",
      "=======\n",
      "  Dfcotlinhiir noboter p\"\n",
      "\n",
      " on orke thHnnea'n'in roe-vad thithascesifv yevo,. Suli hat\n",
      " bhes ribyeter\n",
      " thingre , bceull cjhelt ato tole surighres, hyhany.\n",
      "\n",
      "\n",
      "  elnerid ul net. Houlf. asd aner' cumerin'\n",
      "=======\n",
      "\n",
      "+ iter 3000\n",
      "+ loss: 67.594496\n",
      "+ perplexity: 226942907477052705133293469696.000000\n",
      "\n",
      "=======\n",
      "ye rant car osreraeleit su\"F\"Mhaf itete of amag ofar mel ye, Ticit' at bo ante if-ominde totiy do h ail we\n",
      "\n",
      " oof nows weokes-l\n",
      " moic dis'. geich at tI ind u. he t I\"diriEker waf theretous meme\n",
      "\n",
      " I wen\n",
      "=======\n",
      "\n",
      "+ iter 3100\n",
      "+ loss: 67.012961\n",
      "+ perplexity: 126870104313048370173207117824.000000\n",
      "\n",
      "=======\n",
      " wos Ire tinad wer an\" I\n",
      " Taaf ceus sHe obs ing that tut Arowid cs norm ouwim, \"apte le Fasst\" he I cfecerlo m bek meg me lo  bicitloe man nthavt.v\"\"Lhe pad ivses thond\n",
      " ly bar Aon the orete ou he sy \n",
      "=======\n",
      "\n",
      "+ iter 3200\n",
      "+ loss: 66.379391\n",
      "+ perplexity: 67329188233681332001835057152.000000\n",
      "\n",
      "=======\n",
      "th tid.\"Ha hiktarg. an pan\n",
      "e fpbewarl on is in6 theg as ebed tarkeed the ne\n",
      " wiDaek ens. I stl thas ard ls pallimtonorpsoan ofethe thimene ontroclar puf on hes, whillen taled dheblowerd.\n",
      " Ash arpat. H\n",
      "=======\n",
      "\n",
      "+ iter 3300\n",
      "+ loss: 65.898772\n",
      "+ perplexity: 41636372474697639816108965888.000000\n",
      "\n",
      "=======\n",
      "h to murlefe om. \"shimamesd dore atf ho srf nr and, him omeinuct yar thwiss sicu-to.\n",
      "\n",
      " Sho ny nheon\n",
      " he thitg,a. Here led m bos tfel han\n",
      " wan toved ard al aqu bipiretualrr the ll at we oon marlon s in\n",
      "=======\n",
      "\n",
      "+ iter 3400\n",
      "+ loss: 65.589065\n",
      "+ perplexity: 30547045189348943978383278080.000000\n",
      "\n",
      "=======\n",
      "nd an anl sovendes bomner thid teds, thette Gipo il met pon\n",
      " us miundt ho on tho\n",
      " and ou\n",
      " yhein ypeve tcorree\"\n",
      " \"joubris bpf bro thsy co mhe thuy iith ncan sed pard hitortey mis an amat beed llios han\n",
      "=======\n",
      "\n",
      "+ iter 3500\n",
      "+ loss: 65.123536\n",
      "+ perplexity: 19177512398085590533367922688.000000\n",
      "\n",
      "=======\n",
      "rilsedec  hrad rpiml, ar,l. and lid tighe Those thatimiensifh nou?d bokrer aveongrhuchipnte\n",
      " cfakesdal Muripo wofs te angenlend\n",
      "\n",
      "\n",
      "\n",
      " af he sag opt? sowetim meu himrrhe the mid oturg.\n",
      " t flo. nhsisive i\n",
      "=======\n",
      "\n",
      "+ iter 3600\n",
      "+ loss: 64.852049\n",
      "+ perplexity: 14617966387015625942149103616.000000\n",
      "\n",
      "=======\n",
      " taf btender axnm\"oum thu ofant  haseset onchisdade; ic ad the in thitm cas Drithe the tometg an.\"\"5-4f48âJ*ŸG´Z¨€ÂBŸ€½™¨E¨E™£`Ÿ°¨ªQ:08»3]±âŸ½[°5¢¶1GŸ°£½£*F¢âŸÃY¨[®Ÿª¨4Ã¨°ª±P±Ã°®€™£€4QÂ[»´»Â[ŸÃ±`™¼Ÿ&´\n",
      "=======\n",
      "\n",
      "+ iter 3700\n",
      "+ loss: 64.534743\n",
      "+ perplexity: 10643453985150257478991085568.000000\n",
      "\n",
      "=======\n",
      "to,e th bir\n",
      " sho, lowangimye xupors eo Therlag aed to hicleatyorleo ko re biveind ared ny.ing dow wavith is\n",
      " if he tacsicos angelke co me, golCle'tl ele osd alyan\n",
      "\n",
      " risin fe singere enm ofr boo ales h\n",
      "=======\n",
      "\n",
      "+ iter 3800\n",
      "+ loss: 64.234662\n",
      "+ perplexity: 7884228067074883917103235072.000000\n",
      "\n",
      "=======\n",
      "udituthe acisr ar genlo fos retad yelimy omed It tus rent ire f'n'd\n",
      " kurledde he thrctutedslecked. \"As bed to foMe toun canlet anonde cborlly tos  erer aok angins Lou noacdert biikdere oi,erbethe fece\n",
      "=======\n",
      "\n",
      "+ iter 3900\n",
      "+ loss: 64.093777\n",
      "+ perplexity: 6848158237965522924103270400.000000\n",
      "\n",
      "=======\n",
      " TorcepetherLer and has thellanteot wes ait, bhe meis so coou I she cag me ts fug gin me sory atand oul he. Hohe' tild, tharelde\n",
      " thye. as anang ord ronidt pyin indre yad bhe nouses\n",
      " mot\n",
      " er, driot he\n",
      "=======\n",
      "\n",
      "+ iter 4000\n",
      "+ loss: 63.972583\n",
      "+ perplexity: 6066520887259681461668675584.000000\n",
      "\n",
      "=======\n",
      "el ay wonyhung Aong able sandid hicrun Hay \"'nciirr. aritsen sivh mpoe  hed bou he birt hn stiulc whis vond menchanhowyatt smeus\"\n",
      " Ohink ee fon. dherort, icovath soame mesetice\n",
      " the te Whemf. dofi ind\n",
      "=======\n",
      "\n",
      "+ iter 4100\n",
      "+ loss: 63.632327\n",
      "+ perplexity: 4316865798254985132936003584.000000\n",
      "\n",
      "=======\n",
      "ed as orifad I ep damend ank by on th satl ssnes was oud mledeby yo udo lond aver.\n",
      " \"TPeedihant if fheked tot red of Yors he thay, cos bachirom,\n",
      "\n",
      "\n",
      " lot, Ian delith' besd,\n",
      " the mosesiry.\"\n",
      "  smteche mo \n",
      "=======\n",
      "\n",
      "+ iter 4200\n",
      "+ loss: 63.310991\n",
      "+ perplexity: 3130503172689165232361177088.000000\n",
      "\n",
      "=======\n",
      "\"\n",
      "\n",
      " Is Hherphacted stmye dowe and mongo-plrate vitn, wooe cored an, fsaleas asesdtr korincake wy \"Soundatseveasilk es itku e'nt sferlerte fofa wangerscalic\"sot chenlgiledhe choace sbalstan\n",
      " woceryayt,\n",
      "=======\n",
      "\n",
      "+ iter 4300\n",
      "+ loss: 63.174529\n",
      "+ perplexity: 2731172894585949156779491328.000000\n",
      "\n",
      "=======\n",
      "de't arr hetod thee'ctor soute\n",
      " weout ow igme that swato te ing\n",
      " Suis wumen paxof sis se ses to erved ans os se f bey morifart Vse sis en. Hwas reeth\n",
      " hee\n",
      " merige waydef hee\n",
      " ent\n",
      " dees. My\n",
      " ard ohe wa\n",
      "=======\n",
      "\n",
      "+ iter 4400\n",
      "+ loss: 62.888084\n",
      "+ perplexity: 2050915964220856562926747648.000000\n",
      "\n",
      "=======\n",
      "adrv sont perer waang ance oesicers th lacf coun\n",
      " wan los ais'thecicn id lare, Haspeyin th oung coahe ofreby ain thomit sotuwet llat mtrt\n",
      " \n",
      " ounsy he othew thiard leantt ky in?\" hos emoud dhoudlous he\n",
      "=======\n",
      "\n",
      "+ iter 4500\n",
      "+ loss: 62.489246\n",
      "+ perplexity: 1376367800972414858381230080.000000\n",
      "\n",
      "=======\n",
      "rt her phe rhing or endemeke frighanboung oom an in ge ty if arKle hot,\n",
      " Hed chowhre das himomagoucoan, \n",
      " Has youur end srliYelying om pywaol srince I acGimed.\n",
      ", sore laold of the togive tletpagilk bo\n",
      "=======\n",
      "\n",
      "+ iter 4600\n",
      "+ loss: 62.218739\n",
      "+ perplexity: 1050158709726794415880536064.000000\n",
      "\n",
      "=======\n",
      "d lre\n",
      " hed ont deile, rime litlerigftelaid mrtor in, che nxmad we pare lartpy-of The bore sof eale neldlcho esy aplle\n",
      ", ben thtre enlabt fondon elg hece thes,r metomt,  he thirs orit hasd ixk Hot\n",
      " tar\n",
      "=======\n",
      "\n",
      "+ iter 4700\n",
      "+ loss: 62.029258\n",
      "+ perplexity: 868889446324660539511275520.000000\n",
      "\n",
      "=======\n",
      "med llad yre' I want nout th of hen annt de twe tunlowidle te brounsoingof\n",
      " uangat bre arerd are the she the sour wees rupmond, llet cor ior sof us ge int  ang Gamfiog be. of as ciplorty shan, d\n",
      " re o\n",
      "=======\n",
      "\n",
      "+ iter 4800\n",
      "+ loss: 61.838776\n",
      "+ perplexity: 718189935101623361710587904.000000\n",
      "\n",
      "=======\n",
      "ighe coleply he mand as tuve brete tou ofsan\n",
      " aw turis thute wathante rances ofpale, toadt in wotred fasume uy puttare\n",
      "inceme hool lsivericouxs plpobe igt The uatithes sute ind met mumeth bet ivare th\n",
      "=======\n",
      "\n",
      "+ iter 4900\n",
      "+ loss: 61.472497\n",
      "+ perplexity: 497927542181633466396835840.000000\n",
      "\n",
      "=======\n",
      " cheit sad, bumJan, oandi to frhirou t hlos indro lord ins out. the pontanmpag pan. He bangt, aAg fawe, ppehuntisf ceand an. wan howy thedtesg., The psep we atat sele thuce har oackechow\n",
      " hu nang oun\n",
      "\n",
      "=======\n",
      "\n",
      "+ iter 5000\n",
      "+ loss: 61.411538\n",
      "+ perplexity: 468480981630058318362836992.000000\n",
      "\n",
      "=======\n",
      "cf hion tad,.\n",
      " Tttis thrsanses\n",
      " oor. Whanm?\n",
      " thegene he the tong arg f serles-till indigh it\"ezkedht poras rr, slsfre, wand whe mrorlound ofedrelknerony of  wererlmstand ran gis\n",
      " loes robe thecigh\n",
      " am\n",
      "=======\n",
      "\n",
      "+ iter 5100\n",
      "+ loss: 61.570873\n",
      "+ perplexity: 549401962815316154298400768.000000\n",
      "\n",
      "=======\n",
      "vnite babceve lagued boc by oon, walr as owaf boz bwis care the, thes armizt, waw.\"doT\n",
      " enmy ere\n",
      " on.\n",
      "d ame haon!, Larkeg\n",
      " th ofl ond weon sket raAd sufavand as da ce anmly fof pllame sfaigt hibd whe,\n",
      "=======\n",
      "\n",
      "+ iter 5200\n",
      "+ loss: 61.328231\n",
      "+ perplexity: 431034654055697620123254784.000000\n",
      "\n",
      "=======\n",
      "ind\"'f ot shend, Fh ohe shas wert lirygilt Abaol thtimel sjloutinke\n",
      " thiglimor kicd iw aryiI th ott ang coind thas Iy, huning sartr\" Ie to wal tourbeng ghe the sreymedter, fbigh of af ctere le cont\n",
      " t\n",
      "=======\n",
      "\n",
      "+ iter 5300\n",
      "+ loss: 61.270420\n",
      "+ perplexity: 406822901791427497690136576.000000\n",
      "\n",
      "=======\n",
      "ouufsibn oof fp. I. \n",
      "\n",
      "\" Ny mride\n",
      " tont\n",
      " thered. The ad dhe mome hur thuse rirl theve watop sarle th thotd bo bethicemothenn of the hqeld\n",
      " ced pti, bat tou vond; in the mte faint,\"\n",
      "\n",
      " \n",
      "''\n",
      " sagkela Comit\n",
      "=======\n",
      "\n",
      "+ iter 5400\n",
      "+ loss: 61.133390\n",
      "+ perplexity: 354726634577863065410207744.000000\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======\n",
      " toytragoel?pabwpivransrorke fhrang andoss. heevot yed She turyle pigt meod iig, lerd ege if ssyime mele fterkis cte rhe Thon.\n",
      " wI sofet ping paroled solt.\n",
      " thiter of enit, sumopgy hlaron bunce. Ybiga\n",
      "=======\n",
      "\n",
      "+ iter 5500\n",
      "+ loss: 60.895188\n",
      "+ perplexity: 279540042268747556125147136.000000\n",
      "\n",
      "=======\n",
      "rale of whendo zavincyanso ghighure fod en ad, ounhoml aunt,itfad arame\" an bes ounsrage  cbar yhitlemt min'de eed.\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ad\n",
      " yared cuhu cery-arery helind hidvounk bet ful aud nanded weced ghpreret je\n",
      "=======\n",
      "\n",
      "+ iter 5600\n",
      "+ loss: 60.746380\n",
      "+ perplexity: 240889394362692461836369920.000000\n",
      "\n",
      "=======\n",
      "t toth Pey te lungestingo fad thu thif southn yan tord pliar waondevet\n",
      " toveres ir weveltoreud sumeess lall her wes. anding comd wan thoon, base thenM\n",
      " ind. Hang\n",
      " gutsis rbetpd\"d\n",
      " the th thens walnigi\n",
      "=======\n",
      "\n",
      "+ iter 5700\n",
      "+ loss: 60.550248\n",
      "+ perplexity: 197987873850529401310019584.000000\n",
      "\n",
      "=======\n",
      "tire praid ket hilous sand\n",
      " tuot antes hoinh rhout samy thithereee. tho cte theo that\n",
      " ot oog asnerere see\n",
      " wy oh ins enpane azevexder cuily PpMom esepmid, rol yous e\n",
      "slab masvent ehess cte und seetwe\n",
      "=======\n",
      "\n",
      "+ iter 5800\n",
      "+ loss: 60.416594\n",
      "+ perplexity: 173218160942162623263145984.000000\n",
      "\n",
      "=======\n",
      "ges\n",
      "\n",
      " wids,, wet tiys Par\n",
      " waviwes ras\n",
      " do dut marsu on wis himre tcoy pal.\n",
      "\n",
      "p\n",
      " llensof med, Ioher rand argiof outlewcealatu dued omeming us tow azgale. \n",
      "\n",
      " Thee cisrrouy enthitem, mand inloud thind th\n",
      "=======\n",
      "\n",
      "+ iter 5900\n",
      "+ loss: 60.387740\n",
      "+ perplexity: 168291481593622843631861760.000000\n",
      "\n",
      "=======\n",
      " the ofrin herten Yines the unting snezive tao nha ha sicrelt and wan yay aseg adu?t ltat th ass bor\n",
      " ouf te rer parl inarg and fned imald acleumppbas\n",
      "e, sar jary tr ofy aed of mainis Are lifn,\n",
      " l\n",
      " lt\n",
      "=======\n",
      "\n",
      "+ iter 6000\n",
      "+ loss: 60.212106\n",
      "+ perplexity: 141183942857636860437463040.000000\n",
      "\n",
      "=======\n",
      "r coin ten barhe  'fentoxs fiperptend ell uet.\"ure eminy at wereeser ppof, whed rem. I\n",
      " uly the thood. Jt-pe.\" \n",
      " al werefepe matu of\n",
      " itsed te he the sou ble, hounle thies on the thar whed. Yath thaed\n",
      "=======\n",
      "\n",
      "+ iter 6100\n",
      "+ loss: 60.042997\n",
      "+ perplexity: 119218163103980123150352384.000000\n",
      "\n",
      "=======\n",
      "hit the ad hott red of ang tave ths arg me nt and hapighe gant as Ie thuf maggred p\"th, sfacincices, iudesede thad thand tat bect Bus of hlerasd moristus ohis lapt toucereds bad ary oleted hare sante \n",
      "=======\n",
      "\n",
      "+ iter 6200\n",
      "+ loss: 59.865824\n",
      "+ perplexity: 99861287960328987706654720.000000\n",
      "\n",
      "=======\n",
      "find sis preldourinket iguus all an moomyire. -hondeedet threlt srant falh wirj,\n",
      " \"Yons,\n",
      " \"Myove they loud os. Il bonge ou fooc as biwith.\n",
      " An of ho ay. These woo ad\n",
      " hachalf samy eigrerd colt a aren \n",
      "=======\n",
      "\n",
      "+ iter 6300\n",
      "+ loss: 59.845378\n",
      "+ perplexity: 97840198033509833238380544.000000\n",
      "\n",
      "=======\n",
      "ed soll, ho bowly\"\n",
      "\n",
      " fsen Dy wuinleg.\n",
      "\n",
      ",\"Read of hem ots Irere Jlad aumetu getd ou lat Of \"s, mho hoy of I ang arend\n",
      " Crelg is chedthrtoun erde heask\n",
      " zus ale misoy wech theng Lhe sumor\n",
      " Ilende as dos\n",
      "=======\n",
      "\n",
      "+ iter 6400\n",
      "+ loss: 59.703792\n",
      "+ perplexity: 84923407989681455856877568.000000\n",
      "\n",
      "=======\n",
      "  in thed Fepete lictas woce, th aut th annind thecn ptheved. \"AUt. I paon haod hens ato wed ohapad wiofow une, hay tierney lath fat a pyreon inrene rit den\n",
      " apbof towhed thed aof oft\n",
      " \"eo wir umturne\n",
      "=======\n",
      "\n",
      "+ iter 6500\n",
      "+ loss: 59.747526\n",
      "+ perplexity: 88719838856963410481905664.000000\n",
      "\n",
      "=======\n",
      " thos. alamese wort ofo.\n",
      " Tlolroreitichs hatd ari ceny howm ow, hipprowers ou amyer vor om hey heremee whe pof rreve yo ung thes\n",
      " chirad. ame\n",
      " the ent band's woun. heu sipseid fpevoy rechibe pe touren\n",
      "=======\n",
      "\n",
      "+ iter 6600\n",
      "+ loss: 59.548087\n",
      "+ perplexity: 72678455391680867984211968.000000\n",
      "\n",
      "=======\n",
      "e silapee. souging wicas hist b zarsrowd\n",
      " duod ohu sel goun ato ghiards uoven, Thed. vhon Bbe ;rhee be mrevus of tf woaps nint hee vint thatnros. ohe if dan dosss ilyerlent\n",
      " cornapngoce.\n",
      "\n",
      " Yyren?\n",
      "\n",
      " \"I\n",
      "=======\n",
      "\n",
      "+ iter 6700\n",
      "+ loss: 59.519846\n",
      "+ perplexity: 70654619850663969251393536.000000\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/h7/tzxfms7d2z7gwlgtbvw15msc0000gn/T/ipykernel_87526/615589767.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/h7/tzxfms7d2z7gwlgtbvw15msc0000gn/T/ipykernel_87526/2442282770.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sample_rate, sample_size)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0;31m# forward seq_length characters through the net and fetch gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWxh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdby\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhprev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m             \u001b[0msmooth_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmooth_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.999\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;31m# print progress and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/h7/tzxfms7d2z7gwlgtbvw15msc0000gn/T/ipykernel_87526/2442282770.py\u001b[0m in \u001b[0;36mcalculate_loss\u001b[0;34m(self, inputs, targets, hprev)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;31m# backprop through tanh nonlinearity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mdhraw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m             \u001b[0mdbh\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdhraw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m             \u001b[0mdWxh\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdhraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mdWhh\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdhraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rnn.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64d89e00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Tho\\nd Hed ve, hrous tou.\\n Fhlos govlen, parinile te sther'raghe gorit, eng at mur\\n thindhat -ped\\n wietrininay on hivsery int in napis uvaoshed faing, ,rang gio ,ang chy of the ccs tho rhe\\n th hew, bovi\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.generate('T')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947e02b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
